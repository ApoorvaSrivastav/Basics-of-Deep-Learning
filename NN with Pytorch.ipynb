{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The MNIST datasets are hosted on yann.lecun.com that has moved under CloudFlare protection\n",
    "# Run this script to enable the datasets download\n",
    "# Reference: https://github.com/pytorch/vision/issues/1938\n",
    "\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms   #torchvision has datasets related to CV\n",
    "# define a transform to normalize/pre-process the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,)),])\n",
    "\n",
    "# Download and divide data into batches\n",
    "train_data = datasets.MNIST('~/.pytorch/MNIST_data/',download=True,train=True,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_data,batch_size=64,shuffle=True) #converts data into batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "e\n",
      "i\n",
      "o\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "#side note\n",
    "# list of vowels\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "vowels_iter = iter(vowels)\n",
    "\n",
    "print(next(vowels_iter))    # 'a'\n",
    "print(next(vowels_iter))    # 'e'\n",
    "print(next(vowels_iter))    # 'i'\n",
    "print(next(vowels_iter))    # 'o'\n",
    "print(next(vowels_iter))    # 'u'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#accessing the training set batch by batch\n",
    "data_iter = iter(trainloader)\n",
    "images,labels = data_iter.next()\n",
    "print(type(images),images.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a00390908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[45].numpy().squeeze(),cmap='Greys_r')\n",
    "#plt.imshow(images[45].numpy().squeeze(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(weights,features,bias):\n",
    "    out= sigmoid_activation(torch.add(torch.mm(features,weights) , bias))\n",
    "    return out\n",
    "def sigmoid_activation(x):\n",
    "    return 1/(1+torch.exp(-x))   # torch.exp is the tensor version of exp function which imput and output elements of tensor type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Flatten the batch of images images. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(trainloader)\n",
    "for i in range(10):\n",
    "    images,labels = data_iter.next()\n",
    "    images= images.flatten(start_dim=1)\n",
    "    #print(images.shape)\n",
    "\n",
    "# defining the sizes of the layers in the network\n",
    "    input_im= images.shape[0]\n",
    "    features= images.shape[1]\n",
    "    hidden_layer= 256\n",
    "    out=10\n",
    "\n",
    "#initializing the weight matrices\n",
    "    Weights1 = torch.randn((features,hidden_layer))\n",
    "    Weights2 = torch.randn((hidden_layer,out))\n",
    "\n",
    "    Bias1= torch.randn((1,hidden_layer))\n",
    "    Bias2= torch.randn((1,out))\n",
    "\n",
    "# Applying inputs to nn\n",
    "    h = sigmoid_activation(torch.add(torch.mm(images,Weights1) , Bias1))\n",
    "    nn_out = torch.add(torch.mm(h,Weights2) , Bias2)\n",
    "    print(nn_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor a with shape (64, 10) and a tensor b with shape (64,), doing a/b will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 64 examples, you only want to divide by one value, the sum in the denominator. So you need b to have a shape of (64, 1). This way PyTorch will divide the 10 values in each row of a by the one value in each row of b. Pay attention to how you take the sum as well. You'll need to define the dim keyword in torch.sum. Setting dim=0 takes the sum across the rows while dim=1 takes the sum across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorter implementation \n",
    "def softmax2(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x),dim=1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "#implementing softmax\n",
    "def softmax(x):\n",
    "    ## TODO: Implement the softmax function here\n",
    "    out=torch.zeros(x.shape)\n",
    "    classes= x.shape[1]\n",
    "    exp_x= torch.exp(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i]= exp_x[i]/torch.sum(exp_x[i])\n",
    "    return out\n",
    "\n",
    "# Here, out should be the output of the network in the previous excercise with shape (64,10)\n",
    "probabilities = softmax2(nn_out)\n",
    "\n",
    "# Does it have the right shape? Should be (64, 10)\n",
    "print(probabilities.shape)\n",
    "# Does it sum to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(torch.exp(nn_out),dim=1).shape)\n",
    "print(torch.sum(torch.exp(nn_out),dim=1).view(-1,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Building neural network using torch.nn and torch.nn.functional module \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without using F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()   #initializing the parent class\n",
    "        \n",
    "        #Defining both layers and functions in terms of transformations\n",
    "        self.hidden = nn.Linear(784,256)  #linear transformation is w*x +b\n",
    "        self.out = nn.Linear(256,10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x= self.hidden(x)\n",
    "        x= self.sigmoid(x)\n",
    "        x= self.out(x)\n",
    "        x= self.softmax(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this bit by bit.\n",
    "\n",
    "class Network(nn.Module):\n",
    "Here we're inheriting from nn.Module. Combined with super().__init__() this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from nn.Module when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to self.hidden. The module automatically creates the weight and bias tensors which we'll use in the forward method. You can access the weight and bias tensors once the network (net) is created with net.hidden.weight and net.hidden.bias.\n",
    "\n",
    "self.output = nn.Linear(256, 10)\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting dim=1 in nn.Softmax(dim=1) calculates softmax across the columns.\n",
    "\n",
    "def forward(self, x):\n",
    "PyTorch networks created with nn.Module must have a forward method defined. It takes in a tensor x and passes it through the operations you defined in the __init__ method.\n",
    "\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "Here the input tensor x is passed through each operation and reassigned to x. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the __init__ method doesn't matter, but you'll need to sequence the operations correctly in the forward method.\n",
    "\n",
    "Now we can create a Network object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0301, -0.0175, -0.0161,  ..., -0.0031, -0.0111,  0.0088],\n",
       "        [ 0.0027, -0.0098, -0.0140,  ..., -0.0071, -0.0210, -0.0013],\n",
       "        [-0.0275,  0.0290,  0.0335,  ..., -0.0332, -0.0142, -0.0270],\n",
       "        ...,\n",
       "        [ 0.0257,  0.0353,  0.0119,  ...,  0.0268, -0.0035, -0.0194],\n",
       "        [-0.0131,  0.0203, -0.0029,  ..., -0.0209, -0.0194,  0.0120],\n",
       "        [-0.0227,  0.0206,  0.0190,  ...,  0.0026,  0.0163, -0.0202]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 1.3197e-02, -3.4711e-02,  1.7848e-02,  1.0903e-02,  2.3454e-02,\n",
       "        -5.7313e-03, -2.3671e-02,  2.3854e-02, -3.4391e-02,  3.5525e-03,\n",
       "         1.5774e-02, -3.5548e-02, -5.7580e-03,  1.0564e-02,  1.2229e-02,\n",
       "         8.9516e-03,  2.7333e-02, -1.1548e-03, -3.4564e-02, -2.5024e-03,\n",
       "        -3.1393e-03,  1.1039e-02, -1.1059e-02,  2.1330e-02,  1.1883e-02,\n",
       "        -6.6643e-04, -3.3877e-02,  2.1874e-03,  2.8244e-02,  1.0060e-02,\n",
       "         9.7700e-03, -1.8463e-02,  1.8976e-02, -1.9900e-02,  3.3600e-02,\n",
       "        -1.6147e-02, -3.3679e-02, -2.4196e-02,  2.9675e-02,  2.3251e-02,\n",
       "         3.6994e-03, -1.5270e-02,  2.3761e-02,  9.9350e-04,  3.0044e-02,\n",
       "         1.1778e-03,  2.0746e-02,  1.7126e-02, -3.4967e-02,  1.3074e-02,\n",
       "        -2.5816e-06, -2.8678e-02,  1.3151e-02,  1.8260e-02,  1.0664e-02,\n",
       "        -2.1808e-02, -1.0786e-02, -1.4538e-02,  1.2295e-02,  1.6038e-02,\n",
       "         9.5513e-03,  2.2620e-02, -1.7838e-02, -1.4744e-03,  1.1633e-02,\n",
       "        -1.4312e-03, -2.8173e-02, -2.8843e-02,  2.9266e-02, -7.6840e-03,\n",
       "         2.8531e-02,  9.3887e-03,  2.9508e-02,  3.4447e-02,  1.4777e-02,\n",
       "         2.7476e-02,  1.4128e-02, -1.8114e-03,  2.1446e-02, -2.0413e-02,\n",
       "        -3.4949e-02,  1.1329e-02,  8.4040e-03, -1.1852e-02,  1.6561e-02,\n",
       "        -1.9793e-03, -3.0021e-02,  2.3024e-02,  9.7102e-03,  5.2434e-03,\n",
       "         2.7002e-02,  1.7215e-02,  2.8065e-03,  1.7186e-02, -2.9715e-02,\n",
       "         1.4527e-02, -5.6739e-03, -6.6246e-03,  1.5302e-02,  2.4386e-02,\n",
       "         2.6915e-02,  3.3886e-02, -3.1510e-02,  4.1478e-03, -5.7401e-03,\n",
       "         2.1004e-03, -2.4101e-02,  1.1895e-02,  3.5276e-02,  1.0919e-02,\n",
       "         3.2936e-02,  6.5648e-03, -2.3776e-02,  2.0249e-02,  2.1646e-02,\n",
       "         3.3464e-02,  1.8445e-02, -1.1868e-02,  2.3103e-02,  1.6119e-02,\n",
       "        -1.4350e-02,  1.0486e-02, -2.8559e-02,  2.4612e-02,  1.0578e-02,\n",
       "         2.9264e-02, -3.1794e-02, -2.7219e-02,  2.4728e-02, -5.4494e-03,\n",
       "        -2.3680e-02, -6.1850e-04, -3.0521e-02,  2.1760e-02,  3.3234e-02,\n",
       "        -5.7261e-03, -3.4769e-03,  2.1735e-02,  2.7258e-03,  2.4503e-02,\n",
       "        -1.8389e-02, -2.8071e-02, -1.9232e-02, -2.7756e-02,  2.9522e-02,\n",
       "         3.3603e-02,  1.6443e-03, -2.8209e-02, -9.9796e-03,  3.5581e-02,\n",
       "        -5.3256e-03,  7.8161e-03, -3.8528e-03,  8.2937e-03, -1.9300e-02,\n",
       "        -2.5946e-02, -1.0712e-02,  1.7645e-02, -1.7003e-02, -7.5422e-03,\n",
       "         4.1792e-03,  3.0000e-02, -1.5161e-02, -1.3488e-02,  2.4009e-02,\n",
       "        -4.2180e-03, -2.4303e-02,  2.1697e-02, -2.6353e-02,  1.6974e-02,\n",
       "        -7.3818e-03, -2.2844e-02, -1.1706e-02, -1.7106e-02, -6.3960e-03,\n",
       "         2.7528e-02,  1.3430e-02, -3.4275e-02, -2.2319e-02, -1.3785e-02,\n",
       "         1.3794e-02,  2.6167e-02, -2.7721e-03,  1.2238e-02, -2.6180e-02,\n",
       "         3.4953e-02,  1.1550e-02, -3.2476e-02,  2.9705e-02,  3.2120e-02,\n",
       "         1.2627e-02, -2.3025e-02,  2.9568e-02,  2.4484e-02, -1.0112e-02,\n",
       "         2.7363e-02, -1.7193e-02,  1.5224e-02,  3.3948e-02, -2.3704e-02,\n",
       "         3.1259e-03,  2.5304e-02,  3.1729e-03,  3.5154e-03,  1.6937e-02,\n",
       "         2.1014e-02, -1.2582e-02,  9.7682e-03, -7.9960e-03,  2.4155e-02,\n",
       "         2.5311e-02, -1.1914e-02,  2.8488e-02, -2.4495e-02,  8.6846e-03,\n",
       "         3.5541e-02, -5.2228e-03,  3.4883e-02, -3.7661e-03,  5.8131e-03,\n",
       "         4.3480e-03, -3.3675e-02,  1.9959e-02,  1.5184e-02,  3.2884e-02,\n",
       "        -2.0372e-02, -1.4561e-02,  3.0642e-02,  1.4708e-02,  8.8867e-03,\n",
       "         2.5322e-03, -9.1099e-03,  3.3590e-02, -5.0742e-03, -8.6196e-03,\n",
       "         8.5581e-03,  5.9720e-03, -5.8362e-03, -3.1525e-02,  3.2377e-02,\n",
       "         1.6308e-02, -9.0322e-04,  8.0761e-03,  2.7776e-03,  1.8074e-02,\n",
       "        -3.9931e-03,  3.5432e-02,  9.9792e-03, -7.2703e-03, -1.6026e-02,\n",
       "        -4.4553e-03, -3.2236e-02, -2.8614e-02, -2.7635e-02,  1.9904e-02,\n",
       "        -2.3654e-02], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PyTorch using torch.nn.functional\n",
    "#This is more common way as PyTorch don't initialize the weight for sigmoid and softmax and\n",
    "#treat them purely as functions\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(self.output(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Network()\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the nn.ReLU module or F.relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.out = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.out(x),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3= NN()\n",
    "model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Initialization of weights\n",
    "For custom initialization, we want to modify these tensors in place. These are actually autograd Variables, so we need to get back the actual tensors with model.fc1.weight.data. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0390,  0.0673,  0.0455,  ...,  0.0751, -0.0633,  0.0750],\n",
       "        [ 0.0783, -0.0471, -0.0348,  ..., -0.0128,  0.0031, -0.0299],\n",
       "        [-0.0286, -0.0676,  0.1302,  ...,  0.0457,  0.0010,  0.0417],\n",
       "        ...,\n",
       "        [ 0.0283, -0.0568, -0.0135,  ..., -0.0261,  0.0762, -0.0394],\n",
       "        [-0.0753, -0.0250, -0.0175,  ..., -0.0487, -0.0141, -0.0362],\n",
       "        [-0.0365, -0.0125,  0.0915,  ..., -0.0788, -0.0638,  0.0364]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fc1.bias.data.fill_(0)\n",
    "model3.fc2.weight.data.normal_(std=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass \n",
    "train_data_iter = iter(trainloader)\n",
    "images, labels = train_data_iter.next()\n",
    "images.resize_(images.shape[0],784)\n",
    "probabs = model3.forward(images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0291, -0.0259, -0.0233,  ..., -0.0304,  0.0287, -0.0176],\n",
      "        [ 0.0076,  0.0232,  0.0179,  ..., -0.0068, -0.0193,  0.0060],\n",
      "        [-0.0089,  0.0178,  0.0091,  ..., -0.0309, -0.0156, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0061, -0.0020,  0.0070,  ...,  0.0005,  0.0276, -0.0083],\n",
      "        [ 0.0284, -0.0119, -0.0218,  ..., -0.0025,  0.0131, -0.0071],\n",
      "        [ 0.0085,  0.0211, -0.0091,  ..., -0.0170, -0.0164,  0.0303]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#An easy way of creating a model is using nn.Sequential\n",
    "\n",
    "#Hyperparameters of our network \n",
    "input_size = 784\n",
    "hidden_layer_sizes = [128,64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size,hidden_layer_sizes[0]),nn.ReLU(),\n",
    "                     nn.Linear(hidden_layer_sizes[0],hidden_layer_sizes[1]), nn.ReLU(),\n",
    "                     nn.Linear(hidden_layer_sizes[1],output_size),nn.Softmax(dim=1))\n",
    "\n",
    "# forward pass \n",
    "train_data_iter = iter(trainloader)\n",
    "images, labels = train_data_iter.next()\n",
    "images.resize_(images.shape[0],784)\n",
    "probabs = model.forward(images)\n",
    "\n",
    "# To access individual layers\n",
    "print(model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also assign names to the layers using ordered Dictionary. It will be useful in accessing the layers\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([('fc1',nn.Linear(input_size,hidden_layer_sizes[0])),\n",
    "                                   ('relu1',nn.ReLU()),\n",
    "                     ('fc2',nn.Linear(hidden_layer_sizes[0],hidden_layer_sizes[1])),\n",
    "                                  ('relu2',nn.ReLU()),\n",
    "                     ('out',nn.Linear(hidden_layer_sizes[1],output_size)),\n",
    "                                   ('softmax',nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0180,  0.0155, -0.0327,  ..., -0.0235, -0.0303, -0.0310],\n",
      "        [-0.0267, -0.0340, -0.0197,  ..., -0.0183,  0.0288,  0.0246],\n",
      "        [-0.0147, -0.0307, -0.0093,  ...,  0.0257, -0.0137,  0.0327],\n",
      "        ...,\n",
      "        [-0.0098,  0.0282,  0.0273,  ..., -0.0160, -0.0021,  0.0312],\n",
      "        [-0.0275,  0.0264,  0.0164,  ...,  0.0231, -0.0138,  0.0252],\n",
      "        [ 0.0015,  0.0219,  0.0064,  ..., -0.0139, -0.0016,  0.0328]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
